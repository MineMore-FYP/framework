{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harmonyreo/framework/blob/master/tf-preprocess_advancedexample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21ljRT7_3xW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2766f662-77d2-41fc-9a8c-31159ebb329a"
      },
      "source": [
        "#https://www.tensorflow.org/tfx/transform/tutorials/TFT_census_example\n",
        "import sys\n",
        "from __future__ import print_function\n",
        "\n",
        "# Confirm that we're using Python 2\n",
        "assert sys.version_info.major is 2, 'Oops, not running Python 2'\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "temp = tempfile.gettempdir()\n",
        "zip, headers = urllib.urlretrieve('https://storage.googleapis.com/tfx-colab-datasets/census.zip')\n",
        "zipfile.ZipFile(zip).extractall(temp)\n",
        "zipfile.ZipFile(zip).close()\n",
        "urllib.urlcleanup()\n",
        "\n",
        "train = os.path.join(temp, 'census/adult.data')\n",
        "test = os.path.join(temp, 'census/adult.test')\n",
        "\n",
        "try:\n",
        "  import tensorflow_transform as tft\n",
        "  import apache_beam as beam\n",
        "except ImportError:\n",
        "  print('Installing TensorFlow Transform.  This will take a minute, ignore the warnings')\n",
        "  !pip install -q tensorflow_transform\n",
        "  print('Installing Apache Beam.  This will take a minute, ignore the warnings')\n",
        "  !pip install -q apache_beam\n",
        "  import tensorflow_transform as tft\n",
        "  import apache_beam as beam\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "from tensorflow_transform.tf_metadata import dataset_schema\n",
        "\n",
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'workclass',\n",
        "    'education',\n",
        "    'marital-status',\n",
        "    'occupation',\n",
        "    'relationship',\n",
        "    'race',\n",
        "    'sex',\n",
        "    'native-country',\n",
        "]\n",
        "NUMERIC_FEATURE_KEYS = [\n",
        "    'age',\n",
        "    'capital-gain',\n",
        "    'capital-loss',\n",
        "    'hours-per-week',\n",
        "]\n",
        "OPTIONAL_NUMERIC_FEATURE_KEYS = [\n",
        "    'education-num',\n",
        "]\n",
        "LABEL_KEY = 'label'\n",
        "\n",
        "RAW_DATA_FEATURE_SPEC = dict(\n",
        "    [(name, tf.FixedLenFeature([], tf.string))\n",
        "     for name in CATEGORICAL_FEATURE_KEYS] +\n",
        "    [(name, tf.FixedLenFeature([], tf.float32))\n",
        "     for name in NUMERIC_FEATURE_KEYS] +\n",
        "    [(name, tf.VarLenFeature(tf.float32))\n",
        "     for name in OPTIONAL_NUMERIC_FEATURE_KEYS] +\n",
        "    [(LABEL_KEY, tf.FixedLenFeature([], tf.string))]\n",
        ")\n",
        "\n",
        "RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
        "    dataset_schema.from_feature_spec(RAW_DATA_FEATURE_SPEC))\n",
        "\n",
        "testing = False\n",
        "if testing:\n",
        "  TRAIN_NUM_EPOCHS = 1\n",
        "  NUM_TRAIN_INSTANCES = 1\n",
        "  TRAIN_BATCH_SIZE = 1\n",
        "  NUM_TEST_INSTANCES = 1\n",
        "else:\n",
        "  TRAIN_NUM_EPOCHS = 16\n",
        "  NUM_TRAIN_INSTANCES = 32561\n",
        "  TRAIN_BATCH_SIZE = 128\n",
        "  NUM_TEST_INSTANCES = 16281\n",
        "\n",
        "# Names of temp files\n",
        "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
        "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
        "EXPORTED_MODEL_DIR = 'exported_model_dir'\n",
        "\n",
        "class MapAndFilterErrors(beam.PTransform):\n",
        "  \"\"\"Like beam.Map but filters out erros in the map_fn.\"\"\"\n",
        "\n",
        "  class _MapAndFilterErrorsDoFn(beam.DoFn):\n",
        "    \"\"\"Count the bad examples using a beam metric.\"\"\"\n",
        "\n",
        "    def __init__(self, fn):\n",
        "      self._fn = fn\n",
        "      # Create a counter to measure number of bad elements.\n",
        "      self._bad_elements_counter = beam.metrics.Metrics.counter(\n",
        "          'census_example', 'bad_elements')\n",
        "\n",
        "    def process(self, element):\n",
        "      try:\n",
        "        yield self._fn(element)\n",
        "      except Exception:  # pylint: disable=broad-except\n",
        "        # Catch any exception the above call.\n",
        "        self._bad_elements_counter.inc(1)\n",
        "\n",
        "  def __init__(self, fn):\n",
        "    self._fn = fn\n",
        "\n",
        "  def expand(self, pcoll):\n",
        "    return pcoll | beam.ParDo(self._MapAndFilterErrorsDoFn(self._fn))\n",
        "  \n",
        "def preprocessing_fn(inputs):\n",
        "  #\"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "  # Since we are modifying some features and leaving others unchanged, we\n",
        "  # start by setting `outputs` to a copy of `inputs.\n",
        "  outputs = inputs.copy()\n",
        "\n",
        "  # Scale numeric columns to have range [0, 1].\n",
        "  for key in NUMERIC_FEATURE_KEYS:\n",
        "    outputs[key] = tft.scale_to_0_1(outputs[key])\n",
        "\n",
        "  for key in OPTIONAL_NUMERIC_FEATURE_KEYS:\n",
        "    # This is a SparseTensor because it is optional. Here we fill in a default\n",
        "    # value when it is missing.\n",
        "    dense = tf.sparse_to_dense(outputs[key].indices,\n",
        "                               [outputs[key].dense_shape[0], 1],\n",
        "                               outputs[key].values, default_value=0.)\n",
        "    # Reshaping from a batch of vectors of size 1 to a batch to scalars.\n",
        "    dense = tf.squeeze(dense, axis=1)\n",
        "    outputs[key] = tft.scale_to_0_1(dense)\n",
        "\n",
        "  # For all categorical columns except the label column, we generate a\n",
        "  # vocabulary but do not modify the feature.  This vocabulary is instead\n",
        "  # used in the trainer, by means of a feature column, to convert the feature\n",
        "  # from a string to an integer id.\n",
        "  for key in CATEGORICAL_FEATURE_KEYS:\n",
        "    tft.vocabulary(inputs[key], vocab_filename=key)\n",
        "\n",
        "  # For the label column we provide the mapping from string to index.\n",
        "  table = tf.contrib.lookup.index_table_from_tensor(['>50K', '<=50K'])\n",
        "  outputs[LABEL_KEY] = table.lookup(outputs[LABEL_KEY])\n",
        "\n",
        "  return outputs\n",
        "\n",
        "\n",
        "\n",
        "def transform_data(train_data_file, test_data_file, working_dir):\n",
        "  #Transform the data and write out as a TFRecord of Example protos.\n",
        "\n",
        "  #Read in the data using the CSV reader, and transform it using a\n",
        "  #preprocessing pipeline that scales numeric data and converts categorical data\n",
        "  #from strings to int64 values indices, by creating a vocabulary for each\n",
        "  #category.\n",
        "\n",
        "  #Args:\n",
        "    #train_data_file: File containing training data\n",
        "    #test_data_file: File containing test data\n",
        "    #working_dir: Directory to write transformed data and metadata to\n",
        "  #\"\"\"\n",
        "\n",
        "  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
        "  # of the block.\n",
        "  with beam.Pipeline() as pipeline:\n",
        "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "      # Create a coder to read the census data with the schema.  To do this we\n",
        "      # need to list all columns in order since the schema doesn't specify the\n",
        "      # order of columns in the csv.\n",
        "      ordered_columns = [\n",
        "          'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "          'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "          'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
        "          'label'\n",
        "      ]\n",
        "      converter = tft.coders.CsvCoder(ordered_columns, RAW_DATA_METADATA.schema)\n",
        "\n",
        "      # Read in raw data and convert using CSV converter.  Note that we apply\n",
        "      # some Beam transformations here, which will not be encoded in the TF\n",
        "      # graph since we don't do the from within tf.Transform's methods\n",
        "      # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
        "      # to get data into a format that the CSV converter can read, in particular\n",
        "      # removing spaces after commas.\n",
        "      #\n",
        "      # We use MapAndFilterErrors instead of Map to filter out decode errors in\n",
        "      # convert.decode which should only occur for the trailing blank line.\n",
        "      raw_data = (\n",
        "          pipeline\n",
        "          | 'ReadTrainData' >> beam.io.ReadFromText(train_data_file)\n",
        "          | 'FixCommasTrainData' >> beam.Map(\n",
        "              lambda line: line.replace(', ', ','))\n",
        "          | 'DecodeTrainData' >> MapAndFilterErrors(converter.decode))\n",
        "\n",
        "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
        "      # the schema to read the CSV data, but we also need it to interpret\n",
        "      # raw_data.\n",
        "      raw_dataset = (raw_data, RAW_DATA_METADATA)\n",
        "      transformed_dataset, transform_fn = (\n",
        "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n",
        "      transformed_data, transformed_metadata = transformed_dataset\n",
        "      transformed_data_coder = tft.coders.ExampleProtoCoder(\n",
        "          transformed_metadata.schema)\n",
        "\n",
        "      _ = (\n",
        "          transformed_data\n",
        "          | 'EncodeTrainData' >> beam.Map(transformed_data_coder.encode)\n",
        "          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n",
        "\n",
        "      # Now apply transform function to test data.  In this case we remove the\n",
        "      # trailing period at the end of each line, and also ignore the header line\n",
        "      # that is present in the test data file.\n",
        "      raw_test_data = (\n",
        "          pipeline\n",
        "          | 'ReadTestData' >> beam.io.ReadFromText(test_data_file,\n",
        "                                                   skip_header_lines=1)\n",
        "          | 'FixCommasTestData' >> beam.Map(\n",
        "              lambda line: line.replace(', ', ','))\n",
        "          | 'RemoveTrailingPeriodsTestData' >> beam.Map(lambda line: line[:-1])\n",
        "          | 'DecodeTestData' >> MapAndFilterErrors(converter.decode))\n",
        "\n",
        "      raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n",
        "\n",
        "      transformed_test_dataset = (\n",
        "          (raw_test_dataset, transform_fn) | tft_beam.TransformDataset())\n",
        "      # Don't need transformed data schema, it's the same as before.\n",
        "      transformed_test_data, _ = transformed_test_dataset\n",
        "\n",
        "      _ = (\n",
        "          transformed_test_data\n",
        "          | 'EncodeTestData' >> beam.Map(transformed_data_coder.encode)\n",
        "          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
        "              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n",
        "\n",
        "      # Will write a SavedModel and metadata to working_dir, which can then\n",
        "      # be read by the tft.TFTransformOutput class.\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))\n",
        "      \n",
        "      \n",
        "def _make_training_input_fn(tf_transform_output, transformed_examples,\n",
        "                            batch_size):\n",
        "  #\"\"\"Creates an input function reading from transformed data.\n",
        "\n",
        "  #Args:\n",
        "    #tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    #transformed_examples: Base filename of examples.\n",
        "    #batch_size: Batch size.\n",
        "\n",
        "  #Returns:\n",
        "    #The input function for training or eval.\n",
        "  #\"\"\"\n",
        "  def input_fn():\n",
        "    #\"\"\"Input function for training and eval.\"\"\"\n",
        "    dataset = tf.contrib.data.make_batched_features_dataset(\n",
        "        file_pattern=transformed_examples,\n",
        "        batch_size=batch_size,\n",
        "        features=tf_transform_output.transformed_feature_spec(),\n",
        "        reader=tf.data.TFRecordDataset,\n",
        "        shuffle=True)\n",
        "\n",
        "    transformed_features = dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "    # Extract features and label from the transformed tensors.\n",
        "    transformed_labels = transformed_features.pop(LABEL_KEY)\n",
        "\n",
        "    return transformed_features, transformed_labels\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "def _make_serving_input_fn(tf_transform_output):\n",
        "  #\"\"\"Creates an input function reading from raw data.\n",
        "\n",
        "  #Args:\n",
        "    #tf_transform_output: Wrapper around output of tf.Transform.\n",
        "\n",
        "  #Returns:\n",
        "    #The serving input function.\n",
        "  #\"\"\"\n",
        "  raw_feature_spec = RAW_DATA_METADATA.schema.as_feature_spec()\n",
        "  # Remove label since it is not available during serving.\n",
        "  raw_feature_spec.pop(LABEL_KEY)\n",
        "\n",
        "  def serving_input_fn():\n",
        "    \"\"\"Input function for serving.\"\"\"\n",
        "    # Get raw features by generating the basic serving input_fn and calling it.\n",
        "    # Here we generate an input_fn that expects a parsed Example proto to be fed\n",
        "    # to the model at serving time.  See also\n",
        "    # tf.estimator.export.build_raw_serving_input_receiver_fn.\n",
        "    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "        raw_feature_spec, default_batch_size=None)\n",
        "    serving_input_receiver = raw_input_fn()\n",
        "\n",
        "    # Apply the transform function that was used to generate the materialized\n",
        "    # data.\n",
        "    raw_features = serving_input_receiver.features\n",
        "    transformed_features = tf_transform_output.transform_raw_features(\n",
        "        raw_features)\n",
        "\n",
        "    return tf.estimator.export.ServingInputReceiver(\n",
        "        transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "  return serving_input_fn\n",
        "\n",
        "\n",
        "def get_feature_columns(tf_transform_output):\n",
        "  #\"\"\"Returns the FeatureColumns for the model.\n",
        "\n",
        "  #Args:\n",
        "    #tf_transform_output: A `TFTransformOutput` object.\n",
        "\n",
        "  #Returns:\n",
        "    #A list of FeatureColumns.\n",
        "  #\"\"\"\n",
        "  # Wrap scalars as real valued columns.\n",
        "  real_valued_columns = [tf.feature_column.numeric_column(key, shape=())\n",
        "                         for key in NUMERIC_FEATURE_KEYS]\n",
        "\n",
        "  # Wrap categorical columns.\n",
        "  one_hot_columns = [\n",
        "      tf.feature_column.categorical_column_with_vocabulary_file(\n",
        "          key=key,\n",
        "          vocabulary_file=tf_transform_output.vocabulary_file_by_name(\n",
        "              vocab_filename=key))\n",
        "      for key in CATEGORICAL_FEATURE_KEYS]\n",
        "\n",
        "  return real_valued_columns + one_hot_columns\n",
        "\n",
        "\n",
        "\n",
        "def train_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,\n",
        "                       num_test_instances=NUM_TEST_INSTANCES):\n",
        "  #\"\"\"Train the model on training data and evaluate on test data.\n",
        "\n",
        "  #Args:\n",
        "    #working_dir: Directory to read transformed data and metadata from and to\n",
        "        #write exported model to.\n",
        "    #num_train_instances: Number of instances in train set\n",
        "    #num_test_instances: Number of instances in test set\n",
        "\n",
        "  #Returns:\n",
        "    #The results from the estimator's 'evaluate' method\n",
        "  #\"\"\"\n",
        "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
        "  run_config = tf.estimator.RunConfig()\n",
        "\n",
        "  estimator = tf.estimator.LinearClassifier(\n",
        "      feature_columns=get_feature_columns(tf_transform_output),\n",
        "      config=run_config)\n",
        "\n",
        "  # Fit the model using the default optimizer.\n",
        "  train_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
        "      batch_size=TRAIN_BATCH_SIZE)\n",
        "  estimator.train(\n",
        "      input_fn=train_input_fn,\n",
        "      max_steps=TRAIN_NUM_EPOCHS * num_train_instances / TRAIN_BATCH_SIZE)\n",
        "\n",
        "  # Evaluate model on test dataset.\n",
        "  eval_input_fn = _make_training_input_fn(\n",
        "      tf_transform_output,\n",
        "      os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n",
        "      batch_size=1)\n",
        "\n",
        "  # Export the model.\n",
        "  serving_input_fn = _make_serving_input_fn(tf_transform_output)\n",
        "  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
        "  estimator.export_savedmodel(exported_model_dir, serving_input_fn)\n",
        "\n",
        "  return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "try:\n",
        "  transform_data(train, test, temp)\n",
        "  print('Transform took {:.2f} seconds'.format(time.time() - start))\n",
        "  results = train_and_evaluate(temp)\n",
        "  print('Transform and training took {:.2f} seconds'.format(time.time() - start))\n",
        "  pprint.pprint(results)\n",
        "finally:\n",
        "  # cleanup\n",
        "  import shutil\n",
        "  if os.path.isdir(temp) and not testing:\n",
        "    shutil.rmtree(temp)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing TensorFlow Transform.  This will take a minute, ignore the warnings\n",
            "\u001b[K     |████████████████████████████████| 174kB 2.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8MB 46.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 47.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 17.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 19.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 50.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 57.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 24.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 47.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 25.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 27.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 47.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 25.4MB/s \n",
            "\u001b[?25h  Building wheel for tensorflow-transform (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pydot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for avro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyvcf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for proto-google-cloud-datastore-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for googledatastore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: google-cloud-storage 1.16.1 has requirement google-cloud-core<2.0dev,>=1.0.0, but you'll have google-cloud-core 0.29.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.8 has requirement dill>=0.3.0, but you'll have dill 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-translate 1.5.0 has requirement google-cloud-core<2.0dev,>=1.0.0, but you'll have google-cloud-core 0.29.1 which is incompatible.\u001b[0m\n",
            "Installing Apache Beam.  This will take a minute, ignore the warnings\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0804 14:02:44.888448 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/common.py:51: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0804 14:02:44.903022 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py:283: The name tf.SparseTensorValue is deprecated. Please use tf.compat.v1.SparseTensorValue instead.\n",
            "\n",
            "W0804 14:02:44.909198 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/tf_metadata/schema_utils.py:63: The name tf.SparseFeature is deprecated. Please use tf.io.SparseFeature instead.\n",
            "\n",
            "W0804 14:02:44.915509 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/tf_metadata/schema_utils.py:110: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0804 14:02:44.917552 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/tf_metadata/schema_utils.py:120: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0804 14:02:45.330826 139757333882752 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/mappers.py:117: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0804 14:02:45.363642 139757333882752 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/tf_utils.py:343: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0804 14:02:45.464452 139757333882752 deprecation.py:323] From <ipython-input-4-e3514309e114>:134: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0804 14:02:45.519732 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/impl.py:240: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0804 14:02:45.751168 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py:378: The name tf.saved_model.signature_def_utils.predict_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.predict_signature_def instead.\n",
            "\n",
            "W0804 14:02:45.752720 139757333882752 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0804 14:02:45.758610 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py:380: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W0804 14:02:52.252432 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/saved/saved_transform_io.py:230: The name tf.saved_model.constants.ASSETS_KEY is deprecated. Please use tf.saved_model.ASSETS_KEY instead.\n",
            "\n",
            "W0804 14:02:55.627355 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\t\\n\\007Const:0\\022\\004race\"\n",
            "\n",
            "W0804 14:02:55.629067 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022\\noccupation\"\n",
            "\n",
            "W0804 14:02:55.630141 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_8:0\\022\\003sex\"\n",
            "\n",
            "W0804 14:02:55.637331 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_9:0\\022\\016marital-status\"\n",
            "\n",
            "W0804 14:02:55.639585 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_12:0\\022\\014relationship\"\n",
            "\n",
            "W0804 14:02:55.644140 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_13:0\\022\\teducation\"\n",
            "\n",
            "W0804 14:02:55.646487 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_16:0\\022\\tworkclass\"\n",
            "\n",
            "W0804 14:02:55.648469 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_17:0\\022\\016native-country\"\n",
            "\n",
            "W0804 14:02:55.681551 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/tft_beam_io/transform_fn_io.py:39: The name tf.gfile.IsDirectory is deprecated. Please use tf.io.gfile.isdir instead.\n",
            "\n",
            "W0804 14:02:55.683247 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/tft_beam_io/transform_fn_io.py:41: The name tf.gfile.ListDirectory is deprecated. Please use tf.io.gfile.listdir instead.\n",
            "\n",
            "W0804 14:02:55.684619 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/beam/tft_beam_io/transform_fn_io.py:45: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W0804 14:02:56.143506 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\t\\n\\007Const:0\\022\\004race\"\n",
            "\n",
            "W0804 14:02:56.144864 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022\\noccupation\"\n",
            "\n",
            "W0804 14:02:56.149972 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_8:0\\022\\003sex\"\n",
            "\n",
            "W0804 14:02:56.154156 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_9:0\\022\\016marital-status\"\n",
            "\n",
            "W0804 14:02:56.157305 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_12:0\\022\\014relationship\"\n",
            "\n",
            "W0804 14:02:56.160809 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_13:0\\022\\teducation\"\n",
            "\n",
            "W0804 14:02:56.164273 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_16:0\\022\\tworkclass\"\n",
            "\n",
            "W0804 14:02:56.167505 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_17:0\\022\\016native-country\"\n",
            "\n",
            "W0804 14:02:56.242985 139757333882752 tfrecordio.py:57] Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
            "W0804 14:02:56.813301 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\t\\n\\007Const:0\\022\\004race\"\n",
            "\n",
            "W0804 14:02:56.814876 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022\\noccupation\"\n",
            "\n",
            "W0804 14:02:56.819140 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_8:0\\022\\003sex\"\n",
            "\n",
            "W0804 14:02:56.822884 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_9:0\\022\\016marital-status\"\n",
            "\n",
            "W0804 14:02:56.825913 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_12:0\\022\\014relationship\"\n",
            "\n",
            "W0804 14:02:56.827348 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_13:0\\022\\teducation\"\n",
            "\n",
            "W0804 14:02:56.829387 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_16:0\\022\\tworkclass\"\n",
            "\n",
            "W0804 14:02:56.831195 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_17:0\\022\\016native-country\"\n",
            "\n",
            "W0804 14:02:57.296443 139757333882752 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/tensorflow_transform/output_wrapper.py:85: The name tf.saved_model.constants.ASSETS_DIRECTORY is deprecated. Please use tf.saved_model.ASSETS_DIRECTORY instead.\n",
            "\n",
            "W0804 14:02:57.303266 139757333882752 estimator.py:1811] Using temporary folder as model directory: /tmp/tmpYJzY5L\n",
            "W0804 14:02:57.314831 139757333882752 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0804 14:02:57.335365 139757333882752 deprecation.py:323] From <ipython-input-4-e3514309e114>:266: make_batched_features_dataset (from tensorflow.contrib.data.python.ops.readers) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.make_batched_features_dataset(...)`.\n",
            "W0804 14:02:57.349195 139757333882752 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/data/experimental/ops/readers.py:835: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0804 14:02:57.376375 139757333882752 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/data/experimental/ops/readers.py:212: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0804 14:02:57.398902 139757333882752 deprecation.py:323] From <ipython-input-4-e3514309e114>:268: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Transform took 12.37 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0804 14:03:13.750953 139757333882752 deprecation.py:323] From <ipython-input-4-e3514309e114>:375: export_savedmodel (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function has been renamed, use `export_saved_model` instead.\n",
            "W0804 14:03:13.840204 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\t\\n\\007Const:0\\022\\004race\"\n",
            "\n",
            "W0804 14:03:13.841583 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022\\noccupation\"\n",
            "\n",
            "W0804 14:03:13.843472 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_8:0\\022\\003sex\"\n",
            "\n",
            "W0804 14:03:13.844502 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_9:0\\022\\016marital-status\"\n",
            "\n",
            "W0804 14:03:13.847141 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_12:0\\022\\014relationship\"\n",
            "\n",
            "W0804 14:03:13.849462 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_13:0\\022\\teducation\"\n",
            "\n",
            "W0804 14:03:13.851969 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_16:0\\022\\tworkclass\"\n",
            "\n",
            "W0804 14:03:13.853526 139757333882752 ops.py:6619] Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\014\\n\\nConst_17:0\\022\\016native-country\"\n",
            "\n",
            "W0804 14:03:15.053364 139757333882752 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0804 14:03:16.873748 139757333882752 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/metrics_impl.py:2027: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0804 14:03:17.132963 139757333882752 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
            "W0804 14:03:17.158381 139757333882752 metrics_impl.py:804] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Transform and training took 49.93 seconds\n",
            "{'accuracy': 0.81100667,\n",
            " 'accuracy_baseline': 0.7591057,\n",
            " 'auc': 0.8556907,\n",
            " 'auc_precision_recall': 0.9477027,\n",
            " 'average_loss': 0.4387927,\n",
            " 'global_step': 4071,\n",
            " 'label/mean': 0.7591057,\n",
            " 'loss': 0.4387927,\n",
            " 'precision': 0.87163675,\n",
            " 'prediction/mean': 0.7436525,\n",
            " 'recall': 0.8807347}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}